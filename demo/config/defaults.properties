#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

## can set here
## baseRegistryFolder=/..demo/registry/
## baseConfigFolder=/..demo/config/
## baseDataFolder=/..demo/data/

## default tmp persistence for session based objects and hive support
## may need to make it dataset specific (based on data source ...
global.logging=true
global.cloudStorage=false
## global.hiveSupport is read but not used.... will deprecate
global.hiveSupport=false
global.defaultTmpContainer=/tmp
global.defaultContainer=/tmp/stitchr
global.overrideDefaultContainer=true
global.defaultFileType=parquet
## EXPERIMENTAL not active yet
global.globalTempDbEnabled=false
## global.databricks=false
global.databricksHiveRegistration=false

## spark (INFO, WARN, ERROR)
spark.logLevel=ERROR
app.logLevel=WARN

## catalog parameters
## "dc" means postgres DC "registry" means file-based registry
dc.persistence=registry
## used if dc.persistence = dc
dc.dbengine=postgresql
dc.driver=org.postgresql.Driver
dc.host=localhost
dc.port=5432
dc.db=dc
dc.user=dc
dc.pwd=dc
dc.sslmode=disable
dc.dbscope=open 
## dc schema object map
## this is going to be pushed to be managed in the actual data catalog ... but to simplify for this iteration
dc.schema=data_catalog
dc.dataset=dataset_v
dc.dataPersistence=data_persistence_v
dc.schemaColumn=schema_column_v
dc.batchGroup=batch_group_v
dc.batchGroupMembers=batch_group_members_v
# this is now the expected value as the change in the DC schema breaks the updates....
# until we can unblock when we close issue #30
dc.update=false

## EXPERIMENTAL
# #concurrency parameters
## concurrent.threadcount=3
## those are used in testing threaded runs on top of spark.
concurrent.threaded=false
concurrent.semaphores=2

